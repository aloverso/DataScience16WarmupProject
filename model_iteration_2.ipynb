{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv('train.csv')\n",
    "titanic_test = pd.read_csv('test.csv')\n",
    "\n",
    "def clean_data(data, age_filler):\n",
    "    data.Age = data.Age.fillna(age_filler)\n",
    "    data.Fare = data.Fare.fillna(data.Fare.median())\n",
    "    data.loc[data.Sex == \"female\", \"Sex\"] = 1\n",
    "    data.loc[data.Sex == \"male\", \"Sex\"] = 0\n",
    "    data.Embarked = data.Embarked.fillna(\"S\")\n",
    "    data.loc[data.Embarked == \"S\", \"Embarked\"] = 0\n",
    "    data.loc[data.Embarked == \"C\", \"Embarked\"] = 1\n",
    "    data.loc[data.Embarked == \"Q\", \"Embarked\"] = 2\n",
    "    return data\n",
    "    \n",
    "titanic = clean_data(titanic, titanic.Age.median())\n",
    "titanic_test = clean_data(titanic_test, titanic.Age.median())\n",
    "print \"cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_port(port):\n",
    "    if port == 0: # port is s=0, c=1, q=2\n",
    "        return 1,0,0\n",
    "    elif port == 1:\n",
    "        return 0,1,0\n",
    "    elif port == 2:\n",
    "        return 0,0,1\n",
    "\n",
    "def make_embarked_cols(data):\n",
    "    s = []\n",
    "    c = []\n",
    "    q = []\n",
    "\n",
    "    for port in data.Embarked:\n",
    "        sbool, cbool, qbool = parse_port(port)\n",
    "        s.append(sbool)\n",
    "        c.append(cbool)\n",
    "        q.append(qbool)\n",
    "\n",
    "    data['EmbarkedS'] = s\n",
    "    data['EmbarkedC'] = c\n",
    "    data['EmbarkedQ'] = q\n",
    "    \n",
    "    return data\n",
    "\n",
    "make_embarked_cols(titanic)\n",
    "make_embarked_cols(titanic_test)\n",
    "\n",
    "titanic['Child'] = titanic.Age <= 8\n",
    "titanic_test['Child'] = titanic_test.Age <= 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was copied from model_iteration_1 and just allows us to import and clean the data the same way I did in the first iteration.  It also uses my code from my model improvements to add the extra columns I created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793490460157\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"EmbarkedS\", \"EmbarkedQ\", \"EmbarkedC\", \"Child\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830527497194\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are solely based on the DataQuest suggestions of parameters to tune.  It says that the number of trees will improve your model \"up to a point\" - I wonder where that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a7934afdb50c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "fitscores = []\n",
    "for i in range(1,500):\n",
    "    alg = RandomForestClassifier(random_state=1, n_estimators=i, min_samples_split=4, min_samples_leaf=2)\n",
    "    fitscores.append(cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEACAYAAABCl1qQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZFhIkpoMC0lS0/8DzSrgeX9nXMkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f134ac6ae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(fitscores)):\n",
    "    fitscores[i] = fitscores[i].mean()\n",
    "#print fitscores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(fitscores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I learned a couple things from this (besides that it took a loooong time to run, maybe should have skipped every 10 values or so) is that a) I should have made more algorithms at each tree number so it would be less noisy, and b) once you have sufficient number of trees in your forest (here, seems like 50, maybe), adding more doesn't really help too much.  Let's do another one for mins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79910213243546568, 0.79910213243546568, 0.79910213243546568, 0.79910213243546568, 0.79910213243546568, 0.79910213243546568, 0.79910213243546568, 0.79910213243546568, 0.79910213243546568, 0.79910213243546568]\n",
      "[0.83164983164983164, 0.83164983164983164, 0.83164983164983164, 0.83164983164983164, 0.83164983164983164, 0.83164983164983164, 0.83164983164983164, 0.83164983164983164, 0.83164983164983164, 0.83164983164983164]\n",
      "[0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715]\n",
      "[0.81705948372615034, 0.81705948372615034, 0.81705948372615034, 0.81705948372615034, 0.81705948372615034, 0.81705948372615034, 0.81705948372615034, 0.81705948372615034, 0.81705948372615034, 0.81705948372615034]\n",
      "[0.82154882154882147, 0.82154882154882147, 0.82154882154882147, 0.82154882154882147, 0.82154882154882147, 0.82154882154882147, 0.82154882154882147, 0.82154882154882147, 0.82154882154882147, 0.82154882154882147]\n",
      "[0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266]\n",
      "[0.82267115600448937, 0.82267115600448937, 0.82267115600448937, 0.82267115600448937, 0.82267115600448937, 0.82267115600448937, 0.82267115600448937, 0.82267115600448937, 0.82267115600448937, 0.82267115600448937]\n",
      "[0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266, 0.81593714927048266]\n",
      "[0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715, 0.82379349046015715]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nARGREqYkICJSwpQERERK2P8Dpoh7lnEhsY8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77e0d06c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitscores_min = []\n",
    "for i in range(2,20,2):\n",
    "    means = []\n",
    "    for j in range(10): # average of 10 tries\n",
    "        alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=i, min_samples_leaf=i/2)\n",
    "        means.append(cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3).mean())\n",
    "    fitscores_min.append(sum(means)/len(means))\n",
    "    print means # print progress\n",
    "plt.plot(fitscores_min)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values on the bottom are actually 2,4,6,8...18, and it looks like the highest-scoring values are 4 for min split and 2 for min leaf.  Based on these results, I'll go head and use 100 trees and 4/2 for the mins for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n",
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n",
    "\n",
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles\n",
    "\n",
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pd.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is from the DataQuest example - some new columns to improve the model.  This code adds a column for family size, for title, and for which family you come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEwCAYAAAB8ESALAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f134ab71190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"EmbarkedS\", \"EmbarkedQ\", \"EmbarkedC\", \"Child\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see which factors are most important.  I'm really surprised to see Age/Child both low-ranked, based on what I learned from my explorations.  I'm going to select the most important factors based on this: (PClass, Sex, Fare, Title), but also include Child because we know it's important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804713804714\n"
     ]
    }
   ],
   "source": [
    "# Pick only the best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\", \"Child\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818181818182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "predictors_gradboost = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"EmbarkedS\", \"EmbarkedQ\", \"EmbarkedC\", \"Child\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors_gradboost], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:39: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n",
      "{\"O'Sullivan0\": 426, 'Mangan0': 620, 'Lindqvist1': 543, 'Denkoff0': 297, 'Rouse0': 413, 'Berglund0': 207, 'Meo0': 142, 'Arnold-Franchi1': 49, 'Chronopoulos1': 71, 'Skoog5': 63, 'Widener2': 329, 'Pengelly0': 217, 'Goncalves0': 400, 'Myhrman0': 626, 'Beane1': 456, 'Moss0': 104, 'Carlsson0': 610, 'Nicholls2': 136, 'Jussila1': 110, 'Jussila0': 483, 'Long0': 632, 'Wheadon0': 33, 'Connolly0': 261, 'Hansen2': 680, 'Stephenson1': 493, 'Davies0': 336, 'Silven2': 359, 'Vanden Steen0': 311, 'Astor1': 571, 'Patchett0': 480, 'Johanson0': 184, 'Coleridge0': 220, 'Christmann0': 87, 'Carter3': 340, 'Compton2': 665, 'Carter1': 226, 'Turkula0': 414, 'Hassab0': 558, 'Saad0': 566, 'Mellors0': 208, 'Mamee0': 36, 'Madsen0': 119, 'Anderson0': 395, 'Kraeff0': 42, 'Robbins0': 468, 'Lundahl0': 522, 'Gilinski0': 490, 'Porter0': 107, 'Sdycoff0': 352, 'Green0': 204, 'Bishop1': 263, 'Sinkkonen0': 603, 'Otter0': 640, 'Dahl0': 299, 'Troutt0': 582, 'Samaan2': 48, 'Edvardsson0': 553, 'Petroff0': 98, 'Burke0': 134, 'Cardeza1': 556, 'Hawksford0': 599, 'Somerton0': 416, 'Healy0': 248, 'Andersson0': 137, 'Fortune5': 27, 'Andersson6': 14, 'Johnson2': 9, 'Johnson0': 273, 'Coxon0': 91, 'Banfield0': 696, 'McCarthy0': 7, 'Panula5': 50, 'West3': 58, 'Kallio0': 374, 'Hoyt1': 206, 'Hoyt0': 638, 'Mannion0': 589, 'Touma2': 232, 'Futrelle1': 4, 'Jardin0': 507, 'Lemore0': 437, 'Davies2': 460, 'Robert1': 630, 'Butt0': 451, 'Wick2': 286, 'Emanuel0': 628, 'Ibrahim Shawah0': 643, 'Carbines0': 175, 'McEvoy0': 583, 'Moutal0': 75, 'Ling0': 157, 'Watson0': 552, 'Lahoud0': 443, 'Sedgwick0': 302, 'Beesley0': 22, 'Sheerlinck0': 79, 'Hunt0': 218, 'Clifford0': 408, 'Stranden0': 602, 'Ahlin1': 40, 'Woolner0': 55, 'Caram1': 482, 'Sandstrom2': 11, 'Nakid2': 333, 'Frost0': 412, 'Moen0': 73, 'Henry0': 239, 'Persson1': 241, 'Mudd0': 671, 'Glynn0': 32, 'Bowerman1': 312, 'Daniel0': 505, 'Nysten0': 132, 'Saalfeld0': 270, 'Turcin0': 173, 'Nasser1': 10, 'Waelens0': 78, 'Homer0': 502, 'Graham1': 242, 'Graham0': 699, 'Brown0': 177, 'Dorking0': 256, 'Bing0': 72, 'Blank0': 191, 'Tornquist0': 245, 'Olsen1': 180, 'Olsen0': 144, 'Davison1': 304, 'Butler0': 544, 'Calderhead0': 575, 'Cor0': 530, 'Kalvik0': 534, 'Hagland1': 386, 'Boulos2': 131, 'Boulos0': 497, 'Leitch0': 496, 'Endres0': 581, 'Fleming0': 275, 'Natsch1': 247, 'Garfirth0': 614, 'Blackwell0': 300, 'Thayer2': 461, 'Holm0': 657, 'Rekic0': 105, 'Allen0': 5, 'Hendekovic0': 282, 'Svensson0': 424, 'Montvila0': 698, 'Perreault0': 441, 'Francatelli0': 278, 'Gronnestad0': 621, 'Maisner0': 399, 'Radeff0': 537, 'Daly0': 432, 'Jarvis0': 488, 'Bowen0': 515, 'Risien0': 453, 'Walker0': 436, 'Klaber0': 578, 'Wright0': 466, 'Artagaveytia0': 419, 'Navratil2': 138, 'Hays2': 658, 'Seward0': 383, 'Hays0': 279, 'Appleton2': 478, 'Fry0': 654, 'Romaine0': 171, 'de Messemaeker1': 469, 'Gustafsson0': 331, 'Gustafsson2': 101, 'Lulic0': 659, 'Beckwith2': 225, 'Sutton0': 516, 'Cunningham0': 355, 'Behr0': 700, 'Rothschild1': 434, 'Stankovic0': 257, 'Abelson1': 277, 'Lobb1': 230, 'Shellard0': 423, 'Knight0': 593, 'Silverthorne0': 572, \"O'Leary0\": 535, 'Saundercock0': 13, 'Baxter1': 114, 'Elias0': 624, 'Nankoff0': 598, 'Mitkoff0': 533, 'Lines1': 677, 'Burns0': 298, \"O'Driscoll0\": 47, 'Cumings1': 2, 'Culumovic0': 674, 'Penasco y Castellana1': 276, 'Parkes0': 251, 'Duff Gordon1': 467, 'Sloper0': 24, 'Strom2': 228, 'Ringhini0': 327, 'Barah0': 616, 'Richards2': 350, 'Crease0': 67, 'Cavendish1': 600, 'Bjornstrom-Steffansson0': 371, 'Masselmani0': 20, 'Moubarek2': 65, 'Turja0': 555, 'Goldsmith2': 154, 'Fahlstrom0': 210, 'Holverson1': 35, 'Herman3': 510, 'Nenkoff0': 205, 'Nysveen0': 292, 'Eklund0': 617, 'Paulner0': 487, 'Nilsson0': 284, 'Dick1': 563, 'Dantcheff0': 639, 'Swift0': 682, 'Humblen0': 570, 'McCormack0': 661, 'Duane0': 253, 'Ekstrom0': 121, 'Wiseman0': 366, 'Smith0': 160, 'Marvin1': 604, 'Cleaver0': 576, 'Gill0': 683, 'Cameron0': 193, 'Augustsson0': 663, 'Petranec0': 97, 'Stahelin-Maeglin0': 523, 'McDermott0': 80, 'Dean3': 90, 'Laleff0': 691, 'Meyer0': 649, 'Meyer1': 34, 'Johansson0': 100, 'Cacic0': 405, 'Ryerson4': 280, 'Becker3': 167, 'Tobin0': 627, 'Murphy1': 219, 'Goldschmidt0': 93, 'Kink2': 68, 'Weisz1': 125, 'Hold1': 215, 'Stanley0': 420, 'Minahan2': 222, 'Toufik0': 450, 'Minahan1': 354, 'Olsvigen0': 559, 'Odahl0': 307, 'Coleff0': 435, 'Vestrom0': 15, 'Hood0': 70, 'Yousseff0': 421, 'Barton0': 109, 'Icard0': 61, 'Smiljanic0': 148, 'Harmer0': 634, 'Ford4': 84, 'Maioni0': 428, 'Abbing0': 675, 'Oreskovic0': 347, 'Sundman0': 356, 'Kink-Heilmann2': 168, 'Lehmann0': 339, 'Dennis0': 288, 'del Carlo1': 316, 'Najib0': 690, 'Ohman0': 465, 'McCoy2': 272, 'Osen0': 129, 'Pernot0': 166, 'Kent0': 415, 'Turpin1': 41, 'Zabour1': 108, 'Alexander0': 650, 'Palsson4': 8, 'Hogeboom1': 618, 'Keefe0': 404, 'Eustis1': 422, 'Marechal0': 669, 'Uruchurtu0': 30, 'Downton0': 485, 'Ilmakangas1': 591, 'Corn0': 147, 'Parrish1': 236, 'Jerwan0': 406, 'Aks1': 678, 'Lam0': 565, 'Slayter0': 290, 'Weir0': 567, 'Frolicher2': 454, 'Danbom2': 365, 'Alhomaki0': 670, 'Brown2': 548, 'Faunthorpe1': 53, 'Givard0': 195, 'Leyson0': 213, 'Potter1': 692, 'Emir0': 26, 'Slocovski0': 85, 'Celotti0': 86, 'Rood0': 169, 'Betros0': 330, 'Larsson0': 211, 'Andreasson0': 88, 'Vande Walle0': 183, 'Morley0': 396, 'Trout0': 344, 'Thorneycroft1': 372, 'Shorney0': 92, 'Attalah0': 111, 'Ward0': 235, 'Yousif0': 310, 'Wiklund1': 325, 'Thorne0': 233, 'Warren1': 320, 'Milling0': 398, 'Rommetvedt0': 545, 'Pickard0': 370, 'Hassan0': 592, 'Heininen0': 655, 'Bazzani0': 200, 'Plotcharsky0': 335, 'Lindell1': 503, 'Caldwell2': 76, 'Meanwell0': 474, 'Bystrom0': 684, 'Harrison0': 238, 'Goldenberg1': 388, 'Webber0': 117, 'Shelley1': 693, 'Mayne0': 577, 'Honkanen0': 198, 'Karlsson0': 410, 'Calic0': 153, 'Eitemiller0': 538, 'Chapman0': 568, 'Asplund6': 25, 'Peuchen0': 385, 'Cairns0': 244, 'Bailey0': 611, 'Hanna0': 268, 'Garside0': 481, 'Mernagh0': 179, 'Staneff0': 74, 'Toomey0': 393, 'Canavan0': 425, 'Dakic0': 560, 'Foo0': 529, 'Pasic0': 666, 'Novel0': 57, 'Lemberopolous0': 673, 'Hocking4': 625, 'Moran1': 106, 'Abbott2': 252, 'Kvillner0': 377, 'Elias2': 309, 'Doharr0': 476, 'Norman0': 472, 'Leader0': 641, 'Smart0': 402, 'White1': 99, 'Gale1': 348, 'Doling1': 95, 'Moor1': 607, 'Taussig2': 237, 'Pinsky0': 174, 'Gallagher0': 573, 'Markun0': 694, 'de Mulder0': 258, 'Kassem0': 444, 'Yrois0': 182, 'Kantor1': 96, 'Sobey0': 126, 'Shutes0': 506, 'Brocklebank0': 509, 'Cherry0': 234, 'Dooley0': 701, 'Hedman0': 646, 'Madigan0': 181, 'Parr0': 524, 'Campbell0': 401, 'Mullens0': 569, 'Charters0': 363, 'Jonsson0': 477, 'Ostby1': 54, 'Wilhelms0': 551, 'Williams1': 145, 'Williams0': 18, 'Maenpaa0': 221, 'Stone0': 662, 'Lennon1': 46, 'Olsson0': 254, 'Harknett0': 214, 'Horgan0': 508, 'Hart0': 353, 'Hart2': 283, 'Ilett0': 82, 'Baumann0': 156, 'Youseff0': 185, 'Reeves0': 240, 'Thomas1': 645, 'Jensen0': 527, 'Jensen1': 584, 'Cribb1': 150, 'Ross0': 486, 'Andersen-Jensen1': 176, 'Gilnagh0': 146, 'Moran0': 6, 'Coelho0': 123, 'Razi0': 679, 'Reuchlin0': 660, 'Kiernan1': 196, 'Partner0': 295, 'Jermyn0': 322, 'Salkjelsvik0': 103, 'Lahtinen2': 281, 'Drew2': 358, 'Hosono0': 260, 'Danoff0': 289, 'van Billiard2': 143, 'Bracken0': 203, 'Coutts2': 305, 'Connors0': 113, 'Jenkin0': 69, 'Hodges0': 586, 'Vovk0': 442, 'Backstrom1': 188, 'Devaney0': 44, 'Backstrom3': 83, 'Barbara1': 317, \"O'Connell0\": 520, 'Vande Velde0': 608, 'Reynaldo0': 380, 'Isham0': 163, \"O'Brien0\": 463, \"O'Brien1\": 170, 'Giglio0': 130, 'Ponesell0': 644, 'Nicola-Yarred1': 39, 'Angle1': 439, 'Mellinger1': 246, 'Newell1': 197, 'Lang0': 431, 'Davidson1': 549, 'Richard0': 127, 'Ball0': 293, 'Drazenoic0': 122, 'Robins1': 124, 'Richards5': 376, 'Nosworthy0': 51, 'Moore0': 116, 'Newsom2': 128, 'Sjoblom0': 635, 'Zimmerman0': 364, 'Kenyon1': 392, 'Fynney0': 21, 'Laroche3': 43, 'Renouf1': 409, 'Silvey1': 375, 'Christy2': 484, 'Renouf3': 588, 'Theobald0': 612, 'Pekoniemi0': 112, 'Bateman0': 140, 'Spencer1': 31, 'Allison3': 269, 'Cohen0': 186, 'Dimic0': 306, 'Bourke2': 172, 'Farthing0': 447, 'Hamalainen2': 224, 'Karaic0': 504, 'Pettersson0': 648, 'Landergren0': 328, 'Willey0': 532, 'Braund1': 1, 'Slemen0': 652, 'Jalsevac0': 390, 'Harder1': 324, 'Elsbury0': 494, 'Carrau0': 81, 'Sutehall0': 697, 'Sawyer0': 554, 'Vander Planke1': 19, 'Peters0': 557, 'Vander Planke2': 38, 'Dahlberg0': 695, 'Sharp0': 462, 'Sirota0': 667, 'Barkworth0': 521, 'Mack0': 623, 'Sjostedt0': 212, 'Slabenoff0': 499, 'Dodge2': 382, 'Mineff0': 266, 'Hocking3': 449, 'Hickman2': 115, 'Perkin0': 194, 'Stewart0': 64, 'Giles1': 681, 'Lindblom0': 250, 'Meek0': 357, 'Morrow0': 470, 'Newell2': 539, 'Beavan0': 326, 'Balkic0': 688, 'van Melkebeke0': 687, 'Foreman0': 387, 'Hampe0': 378, 'Birkeland0': 351, 'Mockler0': 315, 'Millet0': 391, 'Peter2': 120, 'Yasbeck1': 512, 'Osman0': 642, 'Buss0': 337, 'Sunderland0': 202, 'Lefebre4': 162, 'Bryhl1': 590, 'McGovern0': 314, 'Nye0': 66, 'Davis0': 525, 'Aubart0': 323, 'Spedden2': 287, 'Harris0': 201, 'Harris1': 62, 'McGough0': 433, 'Hewlett0': 16, 'Cann0': 37, 'Mallet2': 656, 'Kirkland0': 517, 'Ridsdale0': 446, 'Connaghton0': 605, 'Stoytcheff0': 475, 'Hippach1': 294, 'Dowdell0': 77, 'Klasen2': 161, 'Rothes0': 613, \"O'Connor0\": 394, 'Ivanoff0': 597, 'Clarke1': 367, 'Scanlan0': 403, 'Troupiansky0': 595, 'Reed0': 227, 'Rintamaki0': 492, 'Chambers1': 587, 'Haas0': 265, 'Greenfield1': 94, 'Jansson0': 341, 'Gheorgheff0': 362, 'Mitchell0': 550, 'Bonnell0': 12, 'Murdlin0': 491, 'Lester0': 651, 'Van Impe2': 361, 'Simonius-Blumer0': 531, 'Torber0': 501, 'Gillespie0': 585, 'Naidenoff0': 259, 'McNamee1': 601, 'de Pelsmaeker0': 255, 'Quick2': 429, 'Byles0': 139, 'Sage10': 149, 'Frolicher-Stehli2': 489, 'Peduzzi0': 389, 'Chibnall1': 155, 'Hakkarainen1': 133, 'Watt0': 151, 'Sirayanian0': 60, 'Leonard0': 165, 'Bissette0': 243, 'Adahl0': 319, 'LeRoy0': 452, 'Tomlin0': 653, 'Kimball1': 513, 'Gavey0': 511, 'Mionoff0': 102, 'Farrell0': 445, 'Goodwin7': 59, 'Strom1': 187, 'Ali0': 192, 'Funk0': 313, 'Van der hoef0': 158, 'Roebling0': 686, 'Simmons0': 473, 'Asim0': 318, 'Rosblom2': 231, 'Lovell0': 209, 'Phillips0': 368, 'Leinonen0': 526, 'Markoff0': 676, 'Levy0': 264, 'Frauenthal2': 540, 'Frauenthal1': 296, 'Baclini3': 384, 'Johannesen-Bratthammer0': 381, 'Jonkoff0': 609, 'Williams-Lambert0': 308, 'Serepeca0': 672, 'Longley0': 518, \"O'Dwyer0\": 28, 'Kelly0': 271, 'Douglas1': 457, 'Crosby2': 455, 'Niskanen0': 345, 'Collander0': 301, 'Gee0': 397, 'Tikkanen0': 334, 'McKane0': 342, 'Molson0': 418, 'Bostandyeff0': 519, 'Ayoub0': 631, 'Rogers0': 45, 'Chip0': 668, 'Cook0': 546, 'Stead0': 229, 'Soholt0': 580, 'Moraweck0': 285, 'Bidois0': 332, 'Taylor1': 547, 'Bradley0': 430, 'Fischer0': 561, 'Barber0': 262, 'Albimona0': 189, 'Hale0': 164, 'Flynn0': 369, 'Sadlier0': 338, 'Keane0': 274, 'Young0': 291, 'Lindahl0': 223, 'Rugg0': 56, 'Johnston3': 633, 'Petterson1': 379, 'Pavlovic0': 440, 'Sivic0': 471, 'Hirvonen1': 411, 'Allum0': 664, 'Harrington0': 500, 'Windelov0': 417, 'Lesurer0': 596, 'Pain0': 343, 'Lievens0': 622, 'Fox0': 303, 'Lewy0': 267, 'McMahon0': 118, 'Greenberg0': 579, 'Heikkinen0': 3, 'Louch1': 373, 'Chapman1': 495, 'Berriman0': 594, 'Moussa0': 321, 'Ryan0': 438, 'Madill1': 562, 'Duran y More1': 685, 'Bengtsson0': 152, 'Harper1': 52, 'Kilgannon0': 629, 'Badt0': 541, 'Salonen0': 448, 'Guggenheim0': 636, 'Widegren0': 349, 'Brewe0': 619, 'Sivola0': 159, 'Nirva0': 615, 'Rice5': 17, 'Andrews0': 647, 'Andrews1': 249, 'Collyer2': 216, 'Strandberg0': 407, 'Colley0': 542, 'Nicholson0': 458, 'Chaffee1': 89, 'Gaskell0': 637, 'Wells2': 606, 'Leeni0': 464, 'Todoroff0': 29, 'Adams0': 346, 'Pears1': 141, 'Sagesser0': 528, 'Carr0': 190, 'McGowan0': 23, 'Hansen0': 514, 'Hansen1': 574, 'Karun1': 564, 'Rush0': 479, 'Matthews0': 360, 'Laitinen0': 427, 'Padro y Manent0': 459, 'Andrew0': 135, 'Vander Cruyssen0': 689, 'Jacobsohn1': 199, 'Lurette0': 178, 'Jacobsohn3': 498, 'Hegarty0': 536}\n",
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "5            897         0\n",
      "6            898         1\n",
      "7            899         0\n",
      "8            900         1\n",
      "9            901         0\n",
      "10           902         0\n",
      "11           903         0\n",
      "12           904         1\n",
      "13           905         0\n",
      "14           906         1\n",
      "15           907         1\n",
      "16           908         0\n",
      "17           909         0\n",
      "18           910         1\n",
      "19           911         1\n",
      "20           912         0\n",
      "21           913         0\n",
      "22           914         1\n",
      "23           915         0\n",
      "24           916         1\n",
      "25           917         0\n",
      "26           918         1\n",
      "27           919         0\n",
      "28           920         0\n",
      "29           921         0\n",
      "..           ...       ...\n",
      "388         1280         0\n",
      "389         1281         0\n",
      "390         1282         0\n",
      "391         1283         1\n",
      "392         1284         1\n",
      "393         1285         0\n",
      "394         1286         0\n",
      "395         1287         1\n",
      "396         1288         0\n",
      "397         1289         1\n",
      "398         1290         0\n",
      "399         1291         0\n",
      "400         1292         1\n",
      "401         1293         0\n",
      "402         1294         1\n",
      "403         1295         0\n",
      "404         1296         0\n",
      "405         1297         0\n",
      "406         1298         0\n",
      "407         1299         0\n",
      "408         1300         1\n",
      "409         1301         1\n",
      "410         1302         1\n",
      "411         1303         1\n",
      "412         1304         1\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pd.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors_gradboost],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"EmbarkedS\", \"EmbarkedC\", \"EmbarkedQ\", \"Child\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions = predictions.astype(int) \n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "print submission\n",
    "\n",
    "submission.to_csv(\"model2_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code from DataQuest, I earned a 0.78947 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to try to tweak some of these models to get a better score.  What I just submitted was an ensemble of the logistic regression and the gradient boost - I wonder what will happen if I make an ensemble of the random forest and the gradient boost - or of all 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830527497194\n",
      "0.818181818182\n",
      "0.81593714927\n",
      "0.817059483726\n",
      "0.820426487093\n",
      "0.820426487093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "def do_ensemble(algorithms):\n",
    "\n",
    "    # Initialize the cross validation folds\n",
    "    kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "    predictions = []\n",
    "    for train, test in kf:\n",
    "        train_target = titanic[\"Survived\"].iloc[train]\n",
    "        full_test_predictions = []\n",
    "        # Make predictions for each algorithm on each fold\n",
    "        for alg, predictors in algorithms:\n",
    "            # Fit the algorithm on the training data.\n",
    "            alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "            # Select and predict on the test fold.  \n",
    "            # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "            test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "            full_test_predictions.append(test_predictions)\n",
    "        # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "        test_predictions = sum(full_test_predictions) / len(full_test_predictions)\n",
    "        # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "        test_predictions[test_predictions <= .5] = 0\n",
    "        test_predictions[test_predictions > .5] = 1\n",
    "        predictions.append(test_predictions)\n",
    "\n",
    "    # Put all the predictions together into one array.\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "    # Compute accuracy by comparing to the training data.\n",
    "    accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# for simplicity sake, I'm going to start by using the predictors_gradboost list, which has pretty much every column of note\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors_gradboost],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2), predictors_gradboost]\n",
    "]\n",
    "algorithms2 = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors_gradboost],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2), predictors_gradboost],\n",
    "    [LogisticRegression(random_state=1), predictors_gradboost]\n",
    "]\n",
    "algorithms3 = [\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2), predictors_gradboost],\n",
    "    [LogisticRegression(random_state=1), predictors_gradboost]\n",
    "]\n",
    "algorithms4 = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors_gradboost],\n",
    "    [LogisticRegression(random_state=1), predictors_gradboost]\n",
    "]\n",
    "algorithms5 = [[GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors_gradboost]]\n",
    "algorithms6 = [[GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors_gradboost],\n",
    "              [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors_gradboost]]\n",
    "\n",
    "print do_ensemble(algorithms)\n",
    "print do_ensemble(algorithms2)\n",
    "print do_ensemble(algorithms3)\n",
    "print do_ensemble(algorithms4)\n",
    "print do_ensemble(algorithms5)\n",
    "print do_ensemble(algorithms6) # 5 and 6 should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was really surprised that the all-three did more poorly than Grad+RF, so I went ahead and calculated all the pairs.  And it turns out, all the ones that do kind of poorly are the logistic regression ones.  Is it because of logistic regression, or because I'm doing bad regression because of all the columns I'm giving it?\n",
    "\n",
    "I'm going to try to do this procedurally.  I have three models I can use: LR, GB, and RF.  I already determined the \"ideal\" parameters for RF so I'll just hold those constant for simplicity's sake.  But I can go ahead and make a bunch of models based on which parameter lists I give them, and then make combinations of these models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803591470258\n",
      "0.803591470258\n",
      "0.79797979798\n",
      "0.79797979798\n",
      "0.801346801347\n",
      "0.801346801347\n",
      "0.79012345679\n",
      "0.79012345679\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.809203142536\n",
      "0.809203142536\n",
      "0.812570145903\n",
      "0.812570145903\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.813692480359\n",
      "0.813692480359\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.812570145903\n",
      "0.812570145903\n",
      "0.819304152637\n",
      "0.819304152637\n",
      "0.822671156004\n",
      "0.822671156004\n",
      "0.822671156004\n",
      "0.822671156004\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.82379349046\n",
      "0.82379349046\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.814814814815\n",
      "0.814814814815\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.811447811448\n",
      "0.811447811448\n",
      "0.803591470258\n",
      "0.803591470258\n",
      "0.813692480359\n",
      "0.813692480359\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.817059483726\n",
      "0.817059483726\n",
      "0.814814814815\n",
      "0.814814814815\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.821548821549\n",
      "0.821548821549\n",
      "0.82379349046\n",
      "0.82379349046\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.806958473625\n",
      "0.812570145903\n",
      "0.812570145903\n",
      "0.819304152637\n",
      "0.819304152637\n",
      "0.822671156004\n",
      "0.822671156004\n",
      "0.822671156004\n",
      "0.822671156004\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.82379349046\n",
      "0.82379349046\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.814814814815\n",
      "0.814814814815\n",
      "0.819304152637\n",
      "0.819304152637\n",
      "0.820426487093\n",
      "0.820426487093\n",
      "0.817059483726\n",
      "0.817059483726\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.820426487093\n",
      "0.820426487093\n",
      "0.826038159371\n",
      "0.826038159371\n",
      "0.824915824916\n",
      "0.824915824916\n",
      "0.820426487093\n",
      "0.820426487093\n",
      "0.824915824916\n",
      "0.824915824916\n",
      "0.820426487093\n",
      "0.820426487093\n",
      "0.826038159371\n",
      "0.826038159371\n",
      "0.821548821549\n",
      "0.821548821549\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.820426487093\n",
      "0.820426487093\n",
      "0.819304152637\n",
      "0.819304152637\n",
      "0.814814814815\n",
      "0.814814814815\n",
      "0.824915824916\n",
      "0.824915824916\n",
      "0.827160493827\n",
      "0.827160493827\n",
      "0.824915824916\n",
      "0.824915824916\n",
      "0.821548821549\n",
      "0.821548821549\n",
      "0.837261503928\n",
      "0.837261503928\n",
      "0.828282828283\n",
      "0.828282828283\n",
      "0.83164983165\n",
      "0.83164983165\n",
      "0.81593714927\n",
      "0.81593714927\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.811447811448\n",
      "0.811447811448\n",
      "0.803591470258\n",
      "0.803591470258\n",
      "0.813692480359\n",
      "0.813692480359\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.817059483726\n",
      "0.817059483726\n",
      "0.814814814815\n",
      "0.814814814815\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.821548821549\n",
      "0.821548821549\n",
      "0.82379349046\n",
      "0.82379349046\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.820426487093\n",
      "0.820426487093\n",
      "0.819304152637\n",
      "0.819304152637\n",
      "0.814814814815\n",
      "0.814814814815\n",
      "0.824915824916\n",
      "0.824915824916\n",
      "0.827160493827\n",
      "0.827160493827\n",
      "0.824915824916\n",
      "0.824915824916\n",
      "0.821548821549\n",
      "0.821548821549\n",
      "0.837261503928\n",
      "0.837261503928\n",
      "0.828282828283\n",
      "0.828282828283\n",
      "0.83164983165\n",
      "0.83164983165\n",
      "0.81593714927\n",
      "0.81593714927\n",
      "0.818181818182\n",
      "0.818181818182\n",
      "0.827160493827\n",
      "0.827160493827\n",
      "0.827160493827\n",
      "0.827160493827\n",
      "0.808080808081\n",
      "0.808080808081\n",
      "0.837261503928\n",
      "0.837261503928\n",
      "0.835016835017\n",
      "0.835016835017\n",
      "0.821548821549\n",
      "0.821548821549\n",
      "0.81593714927\n",
      "0.81593714927\n",
      "0.836139169473\n",
      "0.836139169473\n",
      "0.833894500561\n",
      "0.833894500561\n",
      "0.821548821549\n",
      "0.821548821549\n",
      "0.814814814815\n",
      "0.814814814815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=1)\n",
    "gb = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "all_p = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"EmbarkedS\", \"EmbarkedQ\", \"EmbarkedC\", \"Child\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "best_p = [\"Pclass\", \"Sex\", \"Fare\", \"Title\", \"Child\"]\n",
    "\n",
    "algs = [lr, gb, rf]\n",
    "ps = [all_p, best_p]\n",
    "\n",
    "maxscore = 0\n",
    "maxalgs = \"\"\n",
    "maxps = \"\"\n",
    "\n",
    "#trios\n",
    "for alg1 in algs:\n",
    "    for alg2 in algs:\n",
    "        for alg3 in algs:\n",
    "            for p1 in ps:\n",
    "                for p2 in ps:\n",
    "                    for p3 in ps:\n",
    "                        score = do_ensemble([[alg1, p1], [alg2, p1], [alg3, p2]])\n",
    "                        print score\n",
    "                        if score > maxscore:\n",
    "                            maxscore = score\n",
    "                            maxalgs = str(type(alg1)) + str(type(alg2)) + str(type(alg3))\n",
    "                            maxps = str(len(p1)) + str(len(p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: I am super super aware that this was a horribly inefficient way to figure this out.  It had a ton of repeats and extra unnecessary calculations.  What would be a better way to figure out this information than all these awful nested for loops?  And I was also trying to think of a good way to visually convey this information, but nothing good was coming to mind.  It's just some combination of 3 different variables (so, 7 different order possibilities - we have all 3, 3 different pairs, or 3 single-variable), and for each variable in those it can go one of two ways for the predictors.  It probably just feels more complicated than it is, but if I had more time to really work hard on this kind of extraneous component, I'd like to figure out a good way to represent this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837261503928\n",
      "<class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'><class 'sklearn.ensemble.forest.RandomForestClassifier'><class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "1414\n"
     ]
    }
   ],
   "source": [
    "print maxscore\n",
    "print maxalgs\n",
    "print maxps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little disappointed that it told me the best combination was the one that I had already tried: RF + GB, using all the variables.  But I'm going to go ahead and submit to Kaggle to see what it says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_submission(algorithms, filename): \n",
    "\n",
    "    full_predictions = []\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm using the full training data.\n",
    "        alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "        # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "        predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "        full_predictions.append(predictions)\n",
    "\n",
    "    # The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "    #predictions = (full_predictions[1] * 3 + full_predictions[0]) / 4\n",
    "\n",
    "    predictions[predictions <= 0.5] = 0\n",
    "    predictions[predictions > 0.5] = 1\n",
    "    predictions = predictions.astype(int) \n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "            \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "            \"Survived\": predictions\n",
    "        })\n",
    "\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "lr = LogisticRegression(random_state=1)\n",
    "gb = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "    \n",
    "algorithms = [\n",
    "    [gb, all_p],\n",
    "    [rf, all_p]\n",
    "]\n",
    "make_submission(algorithms, \"model2_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I was really surprised to find that it was _not_ an improvement on my previous submission.  My first submission for iteration 2, which was GB + LR and used all_p for GB, most of the predictors for LR.  This one (RF + GB, all_p) scored 0.78469, which is slightly worse, even though it did significantly better (2%!) in the cross-validation kfold testing on my training data.\n",
    "\n",
    "Then, I realized I had left in the weighting from my original submission that weighted the first algorithm (in this case, GB) higher - so I commented that out was surpised that my score went down to 0.77033"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, how do the variables of GB change it?  Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmWUcCGZmBsD/B2nxvgQ5vkuXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13471c3410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nestim = []\n",
    "\n",
    "for i in range(5,100,5):\n",
    "    tests = []\n",
    "    for j in range(10):     \n",
    "        gb = [[GradientBoostingClassifier(random_state=1, n_estimators=i, max_depth=3), predictors_gradboost]]\n",
    "        tests.append(do_ensemble(gb))\n",
    "    nestim.append(sum(tests)/len(tests))\n",
    "\n",
    "plt.plot(nestim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we went by 5s, we can multiply each x-axis value by 5 to get the best n_estimators.  It looks like 13 is best, which is 65 estimators.  We can try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13470a6f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxdepth = []\n",
    "\n",
    "for i in range(1,15):\n",
    "    tests = []\n",
    "    for j in range(10):     \n",
    "        gb = [[GradientBoostingClassifier(random_state=1, n_estimators=65, max_depth=i), predictors_gradboost]]\n",
    "        tests.append(do_ensemble(gb))\n",
    "    maxdepth.append(sum(tests)/len(tests))\n",
    "\n",
    "plt.plot(maxdepth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really interesting - I wasn't expecting the accuracy to go down with increased max_depth.  So based on this, I'm going to use a max_depth 2 because it's actually slightly higher than the 3 we were using before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thought of something to try: we know that gradient boosting and RF seem to be working better that logistic regression.  What if I made an ensemble of a lot of different RF/GB combos with different parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824915824916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "algs = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), all_p],\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=65, max_depth=2), all_p],\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), best_p],\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=65, max_depth=2), best_p],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2), all_p],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2), best_p]\n",
    "]\n",
    "print do_ensemble(algs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It went down from the single GB/RF combo :( so it seems that adding more variety to my ensemble doesn't actually help it.  Regardless, let's try it out on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_submission(algs, \"model2_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It scored .78469 - still not an improvement on my original submission (the one that DataQuest helped with, the one with logistic regression in the ensemble instead of RF, and the one that did more poorly on my cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, one more thing to try.  Since I guess my original LR/GB combo is best, let's try it again but with the newly determined \"best\" GB parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817059483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=65, max_depth=2), all_p],\n",
    "    [LogisticRegression(random_state=1), all_p]\n",
    "]\n",
    "\n",
    "print do_ensemble(algorithms)\n",
    "make_submission(algorithms, \"model2_4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disappointingly, still not better!  It got 0.77512.  It predicted 77% of the test data correctly.\n",
    "\n",
    "At this point, I have no submissions left for today.  I'm a little disappointed because a lot of the things I tried, I was pretty convinced my optimizations had worked and that it would score higher.  I wonder if the high-scoring cross-validations that got lower scores on the test data did so because they were overfitting?  If that's the case, how can I possibly come up with the best model, if a too-high score means it will score lower on the test data?\n",
    "\n",
    "At least, I feel like I learned from this.  The gradient boost algorithm is still a little fuzzy for me, but I think DataQuest did a really good job explaining what was going on behind the scenes of the random forest, and I really liked that.\n",
    "\n",
    "Ideas for future iterations, I think, would be to learn more algorithms and try them all out.  I feel a little bit like I beat the algorithms over and over (try GB + RF! Optimize GB!  Optimize RF!  Optimize with different predictors of lists! Optimize with different ensemble combinations! etc) and even though some were slightly better/worse than the others, it never varied by more than like 2%, and if I really want to improve this model, I'd need to take a new approach that we haven't learned yet.  I looked through a bunch of other Python scripts and models from the Kaggle collection, and found that most of them were using random forests, so I'm not sure what other models are out there to help get a better score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
